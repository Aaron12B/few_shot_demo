import numpy as np
import torch
from torchvision import transforms, datasets
from resnet12 import ResNet12
import torch.nn.functional as F
import onnx
import onnxruntime as ort

normal_resnet=64
small_resnet=45
tiny_resnet=32

########################################################################


#Variables to change
resnet_type=small_resnet 
path_backbone='C:/Users/aaron/Documents/Aaron/IMTA/IMTA_A3/ProCom/smallmini1.pt5'
dim1_img=32
dim2_img=32
name_onnx="resnet12_{}_{}_{}.onnx".format(dim1_img,dim2_img,resnet_type)


########################################################################


# Fixing the device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")



# Get the model
#model = ResNet12(64, [1,3, 84, 84], 351, True, False).to(device)
model = ResNet12(resnet_type, [3, dim1_img, dim2_img], 64, True, False).to(device)




print("model ResNet12:", model)

def load_model_weights(model, path, device):
    pretrained_dict = torch.load(path, map_location=device)
    model_dict = model.state_dict()
    #pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}
    new_dict = {}
    for k, v in pretrained_dict.items():
        if k in model_dict:
            if 'bn' in k:
                new_dict[k] = v
            else:
                new_dict[k] = v #.half()
    model_dict.update(new_dict) 
    model.load_state_dict(model_dict)
    print('Model loaded!')




#model.load_state_dict(torch.load('/home/r21lafar/Documents/dataset/mini1.pt1', map_location=device))
#model.load_state_dict(torch.load('/hdd/data/backbones/easybackbones/tieredlong1.pt1', map_location=device))
#model.load_state_dict(torch.load('/hdd/data/backbones/easybackbones/mini1.pt1', map_location=device))
load_model_weights(model, path_backbone, device)


# set the model to inference mode
model.train(False)

# Exporting the model to onnx
output_names = [ "Output" ]


batch_size=1
dummy_input = torch.randn(batch_size, 3, dim1_img, dim2_img, device="cpu")
torch.onnx.export(model, dummy_input, name_onnx, verbose=True, opset_version=10, output_names=output_names)

### We will verify, if the exported .onnx model does return the same values as the torch model


# Load the ONNX model
imp_model = onnx.load(name_onnx)

# Check that the model is well formed
print("Check_model:",onnx.checker.check_model(imp_model) )


# Print a human readable representation of the graph
print("Human readable version:")
print(onnx.helper.printable_graph(imp_model.graph))

ort_session = ort.InferenceSession(name_onnx)

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# Compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}
ort_outs = ort_session.run(None, ort_inputs)
print("ort_outs:", ort_outs)

# Comparison with the output of the torch model

dummy_output=model(dummy_input)

print("Exported model has been tested with ONNXRuntime, and the result looks good!")
print("Error:",np.linalg.norm(to_numpy(dummy_output[0])-ort_outs[0]))

from torchsummary import summary

print("Summary:", summary(model, input_size = (3,dim1_img, dim2_img), batch_size = -1,device="cpu"))


np.testing.assert_allclose(to_numpy(dummy_output[0]), ort_outs[0], rtol=1e-02, atol=1e-05)
